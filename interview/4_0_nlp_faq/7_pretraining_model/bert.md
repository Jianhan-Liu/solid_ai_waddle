1. BERT非线性的来源在哪里
    - 前馈层的gelu激活函数和self-attention，self-attention是非线性的(计算score的点积操作)
    
2. bert的mask为何不学习transformer在attention处进行屏蔽score的技巧
    - https://www.zhihu.com/question/318355038
    
3. 在BERT中，token分3种情况做mask，分别的作用是什么
    - todo
    
4. BERT的输入是什么，哪些是必须的，为什么position id不用给，type_id 和 attention_mask没有给定的时候，默认会是什么
    - todo
    
5. 为什么说ELMO是伪双向，BERT是真双向？产生这种差异的原因是什么
    - todo
    
6. BERT和Transformer Encoder的差异有哪些？做出这些差异化的目的是什么
    - todo
    
7. BERT 的两个任务 Masked LM 任务和 Next Sentence Prediction 任务是先后训练的还是交替训练的
    - todo
    
8. bert的位置编码是什么
    - todo

### todo
- 用法
- 蒸馏
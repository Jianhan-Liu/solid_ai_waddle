1. 什么是对比学习CL - Contrastive Learning
    - 对比学习的对象是增强后的数据，即：将原数据进行两次增强，在增强后的数据中挑选任意两条数据，如果这两条数据来自同一条原数据则拉近它们，否则将它们推远（对于一个源数据`x`, 使用某种数据增强的方式，给它生成一对增强数据`x1`和`x2`，然后对它们哥俩进行编码、映射，接着最大化它们的相似度）

1. NLP中常见数据增强的方式
    - 词汇替代：尝试在不更改句子含义的情况下替换文本中出现的单词
        - 基于同义词库的替换
        - 词嵌入替换，通过词向量找寻相近词
        - TF-IDF单词替换：TF-IDF分数较低的单词是无意义的，因此可以替换而不会影响句子的真实标签，不过这种方法效果也一般，因为分数较低的单词对语义的影响也是小的，有些情况下的处理甚至当成停用词去除
        - **Masked语言模型：使用预训练的BERT模型，对文本的某些部分进行遮罩，然后要求BERT模型预测被遮罩token，需要注意的是：决定要掩盖文本的哪一部分并非易事，必须使用启发式方法来确定掩码，否则生成的文本可能不会保留原始句子的含义**（具体操作）
    - 回译
    - 文字表面转换：由于转换不应该改变句子的含义，因此可以看到，在展开歧义语言形式的情况下，这样做可能会失败，中文没有如英文中的缩写语法，因此中文数据不适用
    - 随机噪声注入
        - 拼写错误注入
        - 键盘错误注入
        - Unigram噪声
        - 空白噪声
        - 句子改组
        - 随机插入、交换、删除
    - 实例交叉扩展
        - 假设是，即使结果是不合语法且语义上不合理的，新文本仍将保留情感
    - 语法树操作
    - 文字混合
    - 生成式方法

---
### Filtered Reference
- [对比学习有多火？文本聚类都被刷爆了](https://mp.weixin.qq.com/s/BWK_tKR6If7Lww8SrRB5Ww)
- [丹琦女神新作：对比学习，简单到只需要Dropout两下](https://mp.weixin.qq.com/s/BpbI_S9lXofVFdu8qffIkg)
- [自然语言处理中数据增强（Data Augmentation）技术最全盘点](https://zhuanlan.zhihu.com/p/150600950)
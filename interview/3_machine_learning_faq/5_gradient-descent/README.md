1. 为什么relu比sigmoid更能解决梯度消失

2. 神经网络的梯度下降介绍
    
3. 梯度消失和梯度爆炸的原因与应对策略
    - 原因：梯度连乘效应，大于1的连乘因子导致梯度爆炸，反之导致梯度消失
    - 含义：RNN中梯度消失的含义：距离当前时间步越长，那么其反馈的梯度信号越不显著，最后可能完全没有起作用，这就意味着RNN对长距离语义的捕捉能力失效了
    - 策略：处理梯度爆炸的最根本方法是参数裁剪或梯度裁剪
        - 预训练+微调
        - 梯度剪切、权重正则
        - 使用不同的激活函数
        - BN层
        - 残差结构
        - LSTM网络
    
    
### Filtered Reference
1. [也来谈谈RNN的梯度消失/爆炸问题](https://kexue.fm/archives/7888?sharesource=weibo)
1. 如何区分模型为线性还是非线性模型
    - 在统计意义上，如果一个回归等式是线性的，那么它的相对于参数就必须也是线性的。如果相对于参数是线性，那么即使性对于样本变量的特征是二次方或者多次方，这个回归模型也是线性的。简单判断：看模型的特征因子对应的参数是否超过**1**个，是则是非线性模型。特别的，逻辑回归模型为广义线性模型，因为其分类的边界是一条直线。对比于线性回归是用`wx+b`去拟合数值`y`，对数几率回归则是用`wx+b`去拟合一个几率`ln(y/(1-y))`。
    
2. `KL散度`（相对熵）是严格的距离函数么
    - 不是，其是用来衡量两个分布之间的差异，因为不满足距离的三大性质之一：对称性（A->B == B->A），因此不是严格的距离函数。距离函数的另外两个特性为：非负性（A->B > 0)、三角不等式。余弦距离`（1 - 余弦相似度）`也不是严格的距离函数，其不满足三角不等式性质。
    - 可以通过组合`KL散度`得到既能衡量分布差异也具有对称性的`JS散度`。
    
3. 循环神经网络LSTM单个单元的结构以及其6个公式
    - ![lstm](pics/LSTM.png)
    
4. GRU结构及其公式
    - ![gru](pics/GRU.png)
    
5. 过拟合的对应策略
	- Regularization：数据量比较小会导致模型过拟合, 使得训练误差很小而测试误差特别大。通过在Loss Function 后面加上正则项可以抑制过拟合的产生。缺点是引入了一个需要手动调整的hyper-parameter。
	- Dropout：这也是一种正则化手段，不过跟以上不同的是它通过随机将部分神经元的输出置零来实现。
	- Unsupervised Pre-training：用Auto-Encoder或者RBM的卷积形式一层一层地做无监督预训练, 最后加上分类层做有监督的Fine-Tuning。
	- Transfer Learning（迁移学习）：在某些情况下，训练集的收集可能非常困难或代价高昂。因此，有必要创造出某种高性能学习机（learner），使得它们能够基于从其他领域易于获得的数据上进行训练，并能够在对另一领域的数据进行预测时表现优异。
	
7. batch_size的选择策略
    - 合理的batch_size可以提高内存利用率，大矩阵乘法的并行化效率提高；跑完一个epoch的时间减少，减少相同数据的处理速度；一定范围内，batch_size越大，确定的梯度下降方向越准，引起的训练震荡越小
    - 过大的batch_size导致内存爆满；需要达到同样精度所需的epoch次数增大
    - 过小的batch_size由于各个batch之间的差异性，每次梯度的修正可能互相抵消，即震荡，无法收敛
    
8. torch中参数keepdim的含义
    - 若`keepdim`值为True，则在输出张量中，除了被操作的`dim`维度值降为1，其它维度与输入张量`input`相同。否则，`dim`维度相当于被执行`torch.squeeze()`维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。
    
1. 宏平均和微平均的区别
    - 宏平均每个类的权重相同，微平均每条数据的权重相同
    - 若微平均很小，检查样本量较多的类别；反之检查样本量较少的类别
	
6. 造成线上指标和训练指标差异的原因
    - todo
    
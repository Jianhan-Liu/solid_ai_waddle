1. BN层为什么有效，BN对梯度消失与爆炸的作用
    - 计算过程
        - ![bn](pics/batch-norm.png)
    - 在每层的BP过程中，增加基于标准差的矩阵对权重进行缩放：权重小时方差大，对权重进行放大，避免梯度衰减，反之对权重进行缩小，避免梯度爆炸；整体使得梯度训练可以加快收敛
    - 使得深层网络也可以被学习，原因同上
    - 减少Internal covariate shift，可以使用较大的学习率，提高训练效率
    - 减轻权重初始化对训练的影响
    - 减少模型对正则化的需求
    

2. BN训练和测试时的参数是一样的么
    - BN在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。而在测试时，比如进行一个样本的预测，就并没有batch的概念，因此，这个时候用的均值和方差是全量训练数据的均值和方差，这个可以通过记录训练时的均值和方差，然后通过移动平均法求得。
    - 当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差，gamma和bata

3. 为什么在NLP领域中，LN比BN更合适
    - 如果我们将一批文本组成一个batch，那么BN的操作方向是，对每句话的第一个词进行操作。但语言文本的复杂性是很高的，任何一个词都有可能放在初始位置，且词序可能并不影响我们对句子的理解。而BN是针对每个位置进行缩放，这不符合NLP的规律
    - 由于一个mini batch中的每个句子长度不一致，存在paddding，对列缩放的话会造成误差
    - LN针对单个样本进行，不依赖于其他数据，因此可以避免BN中受到mini-batch数据分布影响
    - LN不需要保存mini-batch的均值和方差，节省了额外的存储空间
   
4. BN vs LN
    - todo
    
5. dropout为什么可以防止过拟合
    - todo

6. dropout与Bagging的差别
    - todo
    
7. dropout有什么缺陷
    - dropout一大缺点就是代价函数**J**不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。定义明确的代价函数**J**每次迭代后都会下降，因为我们所优化的代价函数J实际上并没有明确定义，或者说在某种程度上很难计算，所以我们失去了调试工具来绘制这样的图片。推荐的做法是关闭dropout函数，将keep-prob的值设为1，运行代码，确保**J**函数单调递减。然后打开dropout函数，希望在dropout过程中，代码并未引入bug。

---
### Filtered Reference
1. [深度学习中 Batch Normalization为什么效果好](https://www.zhihu.com/question/38102762/answer/391649040)
2. [详解深度学习中的Normalization，BN/LN/WN](https://zhuanlan.zhihu.com/p/33173246)
3. [Batch Normalization - 李宏毅](https://www.bilibili.com/video/BV1bx411V798?from=search&seid=7639508923003539273)